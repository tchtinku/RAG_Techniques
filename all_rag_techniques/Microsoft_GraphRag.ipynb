{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Microsoft GraphRAG: Enhancing Retrieval-Augmented Generation with Knowledge Graphs\n",
    "\n",
    "#### Overview\n",
    "###### Microsoft GraphRAG is an advanced Retrieval-Augmented Generation (RAG) system that integrates knowledge graphs to improve the performance of large language models (LLMs). Developed by Microsoft Research, GraphRAG addresses limitations in traditional RAG approaches by using LLM-generated knowledge graphs to enhance document analysis and improve response quality.\n",
    "\n",
    "#### Motivation\n",
    "###### Traditional RAG systems often struggle with complex queries that require synthesizing information from disparate sources. GraphRAG aims to: Connect related information across datasets. Enhance understanding of semantic concepts. Improve performance on global sensemaking tasks.\n",
    "\n",
    "#### Key Components\n",
    "###### Knowledge Graph Generation: Constructs graphs with entities as nodes and relationships as edges. Community Detection: Identifies clusters of related entities within the graph. Summarization: Generates summaries for each community to provide context for LLMs. Query Processing: Uses these summaries to enhance the LLM's ability to answer complex questions.\n",
    "\n",
    "#### Method Details\n",
    "#### Indexing Stage\n",
    "\n",
    "###### Text Chunking: Splits source texts into manageable chunks. Element Extraction: Uses LLMs to identify entities and relationships. Graph Construction: Builds a graph from the extracted elements. Community Detection: Applies algorithms like Leiden to find communities. Community Summarization: Creates summaries for each community.\n",
    "\n",
    "#### Query Stage\n",
    "\n",
    "###### Local Answer Generation: Uses community summaries to generate preliminary answers. Global Answer Synthesis: Combines local answers to form a comprehensive response.\n",
    "\n",
    "#### Benefits of GraphRAG\n",
    "###### GraphRAG is a powerful tool that addresses some of the key limitations of the baseline RAG model. Unlike the standard RAG model, GraphRAG excels at identifying connections between disparate pieces of information and drawing insights from them. This makes it an ideal choice for users who need to extract insights from large data collections or documents that are difficult to summarize. By leveraging its advanced graph-based architecture, GraphRAG is able to provide a holistic understanding of complex semantic concepts, making it an invaluable tool for anyone who needs to find information quickly and accurately. Whether you're a researcher, analyst, or just someone who needs to stay informed, GraphRAG can help you connect the dots and uncover new insights.\n",
    "\n",
    "#### Conclusion\n",
    "###### Microsoft GraphRAG represents a significant step forward in retrieval-augmented generation, particularly for tasks requiring a global understanding of datasets. By incorporating knowledge graphs, it offers improved performance, making it ideal for complex information retrieval and analysis.\n",
    "\n",
    "###### For those experienced with basic RAG systems, GraphRAG offers an opportunity to explore more sophisticated solutions, although it may not be necessary for all use cases. Retrieval Augmented Generation (RAG) is often performed by chunking long texts, creating a text embedding for each chunk, and retrieving chunks for including in the LLM generation context based on a similarity search against the query. This approach works well in many scenarios, and at compelling speed and cost trade-offs, but doesn't always cope well in scenarios where a detailed understanding of the text is required.\n",
    "\n",
    "###### GraphRag ( microsoft.github.io/graphrag )\n",
    "\n",
    "###### To run this notebook you can use either OpenAI API key or Azure OpenAI key. Create a .env file and fill in the credentials for your OpenAI or Azure Open AI deployment. The following code loads these environment variables and sets up our AI client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "AZURE_OPENAI_API_KEY=\"\"\n",
    "AZURE_OPENAI_ENDPOINT=\"\"\n",
    "GPT4O_MODEL_NAME=\"gpt-4o\"\n",
    "TEXT_EMBEDDING_3_LARGE_DEPLOYMENT_NAME=\"\"\n",
    "AZURE_OPENAI_API_VERSION=\"2024-06-01\"\n",
    "\n",
    "OPENAI_API_KEY=\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%pip install graphrag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "load_dotenv()\n",
    "from openai import AzureOpenAI, OpenAI\n",
    "\n",
    "AZURE=True # Change to False to use OpenAI\n",
    "if AZURE:\n",
    "   AZURE_OPENAI_API_KEY=os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "   AZURE_OPENAI_ENDPOINT=os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "   GPT4O_DEPLOYMENT_NAME=os.getenv(\"GPT4O_MODEL_NAME\")\n",
    "   TEXT_EMBEDDING_3_LARGE_NAME=os.getenv(\"TEXT_EMBEDDING_3_LARGE_DEPLOYMENT_NAME\")\n",
    "   AZURE_OPENAI_API_VERSION=os.getenv(\"AZURE_OPENAI_API_VERSION\")\n",
    "   oai=AzureOpenAI(azure_endpoint=AZURE_OPENAI_ENDPOINT, api_key=AZURE_OPENAI_API_KEY, api_version=AZURE_OPENAI_API_VERSION)\n",
    "else:\n",
    "    OPENAI_API_KEY=os.getenv(\"OPENAI_API_KEY\")\n",
    "    oai = OpenAI(api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### We'll start by getting a text to work with. The Wikipedia article on Elon Musk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://en.wikipedia.org/wiki/Elon_Musk\"  # Replace with the URL of the web page you want to scrape\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "if not os.path.exists('data'):\n",
    "    os.makedirs('data')\n",
    "\n",
    "if not os.path.exists('data/elon.md'):\n",
    "    elon = soup.text.split('\\nSee also')[0]\n",
    "    with open('data/elon.md', 'w', encoding='utf-8') as f:\n",
    "         f.write(elon)\n",
    "else:\n",
    "   with open('data/elon.md', 'r') as f:\n",
    "        elon = f.read()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### GraphRag has a convenient set of CLI commands we can use. We'll start by configuring the system, then run the indexing operation. Indexing with GraphRag is a much lengthier process, and one that costs significantly more, since rather than just calculating embeddings, GraphRag makes many LLM calls to analyse the text, extract entities, and construct the graph. That's a one-time expense, though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "if not os.path.exists('data/graphrag'):\n",
    "   !python -m graphrag.index --init --root data/graphrag\n",
    "\n",
    "with open('data/graphrag/settings.yaml', 'r') as f:\n",
    "    settings_yaml = yaml.load(f, Loader=yaml.FullLoader)\n",
    "settings_yaml['llm']['model'] = \"gpt-4o\"\n",
    "settings_yaml['llm']['api_key'] = AZURE_OPENAI_API_KEY if AZURE else OPENAI_API_KEY\n",
    "settings_yaml['llm']['type'] = 'azure_openai_chat' if AZURE else 'openai_chat'\n",
    "settings_yaml['embeddings']['llm']['api_key'] = AZURE_OPENAI_API_KEY if AZURE else OPENAI_API_KEY\n",
    "settings_yaml['embeddings']['llm']['type'] = 'azure_openai_embedding' if AZURE else 'openai_embedding'\n",
    "settings_yaml['embeddings']['llm']['model'] = TEXT_EMBEDDING_3_LARGE_NAME if AZURE else 'text-embedding-3-large'\n",
    "if AZURE:\n",
    "        settings_yaml['llm']['api_version'] = AZURE_OPENAI_API_VERSION\n",
    "        settings_yaml['llm']['deployment_name'] = GPT4O_DEPLOYMENT_NAME\n",
    "        settings_yaml['llm']['api_base'] = AZURE_OPENAI_ENDPOINT\n",
    "        settings_yaml['embeddings']['llm']['api_version'] = AZURE_OPENAI_API_VERSION\n",
    "        settings_yaml['embeddings']['llm']['deployment_name'] = TEXT_EMBEDDING_3_LARGE_NAME\n",
    "        settings_yaml['embeddings']['llm']['api_base'] = AZURE_OPENAI_ENDPOINT\n",
    "\n",
    "with open('data/graphrag/settings.yaml', 'w') as f:\n",
    "     yaml.dump(settings_yaml, f)\n",
    "\n",
    "if not os.path.exists('data/graphrag/input'):\n",
    "    os.makedirs('data/graphrag/input')\n",
    "    !cp data/elon.md data/graphrag/input/elon.txt\n",
    "    !python -m graphrag.index --root ./data/graphrag\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### output:  All workflows completed successfully.\n",
    "\n",
    "###### To query GraphRag we'll use its CLI again, making sure to configure it with a context length equivalent to what we use in our embeddings search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import re\n",
    "DEFAULT_RESPONSE_TIME = 'Summarize and explain in 1-2 paragraphs with bullet points using at most 300 tokens'\n",
    "DEFAULT_MAX_CONTEXT_TOKENS = 10000\n",
    "\n",
    "def remove_data(text):\n",
    "    return re.sub(r'\\[Data:.*?\\]', '', text).strip()\n",
    "\n",
    "def ask_graph(query, method):\n",
    "    env = os.environ.copy() | {\n",
    "        'GRAPHRAG_GLOBAL_SEARCH_MAX_TOKENS': str(DEFAULT_MAX_CONTEXT_TOKENS),\n",
    "    }\n",
    "    command = [\n",
    "        'python', '-m', 'graphrag.query',\n",
    "        '--root', './data/graphrag',\n",
    "        '--method', method,\n",
    "        '--response_type', DEFAULT_RESPONSE_TIME,\n",
    "        query,\n",
    "    ]\n",
    "    output = subprocess.check_output(command, universal_newlines=True, env=env, stderr=subprocess.DEVNULL)\n",
    "    return remove_data(output.split('Serach Response: ')[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### GrpahRag offers 2 types of search:\n",
    "\n",
    "###### Global Search for reasoning about holistic questions about the corpus by leveraging the community summaries.\n",
    "###### Local Search for reasoning about specific entities by fanning-out to their neighbors and associated concepts.\n",
    "###### Let's check the local search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import Markdown\n",
    "local_query = \"What and how many companies and subsidieries founded by Elon Musk\"\n",
    "local_result = ask_graph(local_query, 'local')\n",
    "\n",
    "Markdown(local_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "global_query=\"What are the major accomplishments of Elon Musk?\"\n",
    "global_result = ask_graph(global_query, 'global')\n",
    "\n",
    "Markdown(global_result)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
