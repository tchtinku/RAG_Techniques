{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contextual Compression in Document Retrieval\n",
    "#### Overview\n",
    "###### This code demonstrates the implementation of contextual compression in a document retrieval system using LangChain and OpenAI's language models. The technique aims to improve the relevance and conciseness of retrieved information by compressing and extracting the most pertinent parts of documents in the context of a given query.\n",
    "\n",
    "#### Motivation\n",
    "###### Traditional document retrieval systems often return entire chunks or documents, which may contain irrelevant information. Contextual compression addresses this by intelligently extracting and compressing only the most relevant parts of retrieved documents, leading to more focused and efficient information retrieval.\n",
    "\n",
    "#### Key Components\n",
    "###### Vector store creation from a PDF document\n",
    "###### Base retriever setup\n",
    "###### LLM-based contextual compressor\n",
    "###### Contextual compression retriever\n",
    "###### Question-answering chain integrating the compressed retriever\n",
    "\n",
    "#### Method Details\n",
    "###### Document Preprocessing and Vector Store Creation\n",
    "###### The PDF is processed and encoded into a vector store using a custom encode_pdf function.\n",
    "###### Retriever and Compressor Setup\n",
    "###### A base retriever is created from the vector store.\n",
    "###### An LLM-based contextual compressor (LLMChainExtractor) is initialized using OpenAI's GPT-4 model.\n",
    "###### Contextual Compression Retriever\n",
    "###### The base retriever and compressor are combined into a ContextualCompressionRetriever.\n",
    "###### This retriever first fetches documents using the base retriever, then applies the compressor to extract the most relevant information.\n",
    "###### Question-Answering Chain\n",
    "###### A RetrievalQA chain is created, integrating the compression retriever.\n",
    "###### This chain uses the compressed and extracted information to generate answers to queries.\n",
    "\n",
    "#### Benefits of this Approach\n",
    "###### Improved relevance: The system returns only the most pertinent information to the query.\n",
    "###### Increased efficiency: By compressing and extracting relevant parts, it reduces the amount of text the LLM needs to process.\n",
    "###### Enhanced context understanding: The LLM-based compressor can understand the context of the query and extract information accordingly.\n",
    "###### Flexibility: The system can be easily adapted to different types of documents and queries.\n",
    "\n",
    "#### Conclusion\n",
    "###### Contextual compression in document retrieval offers a powerful way to enhance the quality and efficiency of information retrieval systems. By intelligently extracting and compressing relevant information, it provides more focused and context-aware responses to queries. This approach has potential applications in various fields requiring efficient and accurate information retrieval from large document collections.\n",
    "\n",
    "##### import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..'))) # Add the parent directory to the path since we work with notebooks\n",
    "from helper_functions import *\n",
    "from evaluation.evaluate_rag import *\n",
    "\n",
    "# Load environment variables from a .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Set the OpenAI API key environment variable\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define document's path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "path = \"../data/Understanding_Climate_Change.pdf\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "vector_store = encode_pdf(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a retriever + contexual compressor + combine them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Create a retriever\n",
    "retriever = vector_store.as_retriever()\n",
    "\n",
    "# Create a contextual compressor\n",
    "llm = ChatOpenAI(temperature=0, model_name=\"gpt-4o-mini\", max_tokens=4000)\n",
    "compressor = LLMChainExtractor.from_llm(llm)\n",
    "\n",
    "# Combine the retriever with the compressor\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor,\n",
    "    base_retriever=retriever\n",
    ")\n",
    "\n",
    "# Create a QA chain with the compressed retriever\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm = llm,\n",
    "    retriever = compression_retriever,\n",
    "    return_source_documents=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "query = \"What is the main topic of the document?\"\n",
    "result = qa_chain.invoke({\"query\": query})\n",
    "print(result[\"result\"])\n",
    "print(\"Source documents:\", result[\"source_documents\"])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
