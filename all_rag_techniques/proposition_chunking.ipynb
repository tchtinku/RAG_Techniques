{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Propositions Chunking\n",
    "#### Overview\n",
    "###### This code implements the proposition chunking method. The system break downs the input text into propositions that are atomic, factual, self-contained, and concise in nature, encodes the propositions into a vectorstore, which can be later used for retrieval\n",
    "\n",
    "### Key Components\n",
    "###### Document Chunking: Splitting a document into manageable pieces for analysis.\n",
    "###### Proposition Generation: Using LLMs to break down document chunks into factual, self-contained propositions.\n",
    "###### Proposition Quality Check: Evaluating generated propositions based on accuracy, clarity, completeness, and conciseness.\n",
    "###### Embedding and Vector Store: Embedding both propositions and larger chunks of the document into a vector store for efficient retrieval.\n",
    "###### Retrieval and Comparison: Testing the retrieval system with different query sizes and comparing results from the proposition-based model with the larger chunk-based model.\n",
    "\n",
    "#### Motivation\n",
    "###### The motivation behind the propositions chunking method is to build a system that breaks down a text document into concise, factual propositions for more granular and precise information retrieval. Using propositions allows for finer control and better handling of specific queries, particularly for extracting knowledge from detailed or complex texts. The comparison between using smaller proposition chunks and larger document chunks aims to evaluate the effectiveness of granular information retrieval.\n",
    "\n",
    "#### Method Details\n",
    "###### Loading Environment Variables: The code begins by loading environment variables (e.g., API keys for the LLM service) to ensure that the system can access the necessary resources.\n",
    "\n",
    "##### Document Chunking:\n",
    "\n",
    "###### The input document is split into smaller pieces (chunks) using RecursiveCharacterTextSplitter. This ensures that each chunk is of manageable size for the LLM to process.\n",
    "\n",
    "##### Proposition Generation:\n",
    "\n",
    "###### Propositions are generated from each chunk using an LLM (in this case, \"llama-3.1-70b-versatile\"). The output is structured as a list of factual, self-contained statements that can be understood without additional context.\n",
    "\n",
    "##### Quality Check:\n",
    "\n",
    "###### A second LLM evaluates the quality of the propositions by scoring them on accuracy, clarity, completeness, and conciseness. Propositions that meet the required thresholds in all categories are retained.\n",
    "\n",
    "##### Embedding Propositions:\n",
    "\n",
    "###### Propositions that pass the quality check are embedded into a vector store using the OllamaEmbeddings model. This allows for similarity-based retrieval of propositions when queries are made.\n",
    "#### Retrieval and Comparison:\n",
    "\n",
    "###### Two retrieval systems are built: one using the proposition-based chunks and another using larger document chunks. Both are tested with several queries to compare their performance and the precision of the returned results.\n",
    "\n",
    "#### Benefits\n",
    "###### Granularity: By breaking the document into small factual propositions, the system allows for highly specific retrieval, making it easier to extract precise answers from large or complex documents.\n",
    "###### Quality Assurance: The use of a quality-checking LLM ensures that the generated propositions meet specific standards, improving the reliability of the retrieved information.\n",
    "###### Flexibility in Retrieval: The comparison between proposition-based and larger chunk-based retrieval allows for evaluating the trade-offs between granularity and broader context in search results.\n",
    "\n",
    "#### Implementation\n",
    "###### Proposition Generation: The LLM is used in conjunction with a custom prompt to generate factual statements from the document chunks.\n",
    "###### Quality Checking: The generated propositions are passed through a grading system that evaluates accuracy, clarity, completeness, and conciseness.\n",
    "###### Vector Store Integration: Propositions are stored in a FAISS vector store after being embedded using a pre-trained embedding model, allowing for efficient similarity-based search and retrieval.\n",
    "###### Query Testing: Multiple test queries are made to the vector stores (proposition-based and larger chunks) to compare the retrieval performance.\n",
    "\n",
    "#### Summary\n",
    "###### This code presents a robust method for breaking down a document into self-contained propositions using LLMs. The system performs a quality check on each proposition, embeds them in a vector store, and retrieves the most relevant information based on user queries. The ability to compare granular propositions against larger document chunks provides insight into which method yields more accurate or useful results for different types of queries. The approach emphasizes the importance of high-quality proposition generation and retrieval for precise information extraction from complex documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "### LLMs\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from a '.env' file\n",
    "load_dotenv()\n",
    "\n",
    "os.environ['GROQ_API_KEY'] = os.getenv('GROQ_API_KEY') # for LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "sample_content = \"\"\"Paul Graham's essay \"Founder Mode,\" published in September 2024, challenges conventional wisdom about scaling startups, arguing that founders should maintain their unique management style rather than adopting traditional corporate practices as their companies grow.\n",
    "Conventional Wisdom vs. Founder Mode\n",
    "The essay argues that the traditional advice given to growing companies—hiring good people and giving them autonomy—often fails when applied to startups.\n",
    "This approach, suitable for established companies, can be detrimental to startups where the founder's vision and direct involvement are crucial. \"Founder Mode\" is presented as an emerging paradigm that is not yet fully understood or documented, contrasting with the conventional \"manager mode\" often advised by business schools and professional managers.\n",
    "Unique Founder Abilities\n",
    "Founders possess unique insights and abilities that professional managers do not, primarily because they have a deep understanding of their company's vision and culture.\n",
    "Graham suggests that founders should leverage these strengths rather than conform to traditional managerial practices. \"Founder Mode\" is an emerging paradigm that is not yet fully understood or documented, with Graham hoping that over time, it will become as well-understood as the traditional manager mode, allowing founders to maintain their unique approach even as their companies scale.\n",
    "Challenges of Scaling Startups\n",
    "As startups grow, there is a common belief that they must transition to a more structured managerial approach. However, many founders have found this transition problematic, as it often leads to a loss of the innovative and agile spirit that drove the startup's initial success.\n",
    "Brian Chesky, co-founder of Airbnb, shared his experience of being advised to run the company in a traditional managerial style, which led to poor outcomes. He eventually found success by adopting a different approach, influenced by how Steve Jobs managed Apple.\n",
    "Steve Jobs' Management Style\n",
    "Steve Jobs' management approach at Apple served as inspiration for Brian Chesky's \"Founder Mode\" at Airbnb. One notable practice was Jobs' annual retreat for the 100 most important people at Apple, regardless of their position on the organizational chart\n",
    ". This unconventional method allowed Jobs to maintain a startup-like environment even as Apple grew, fostering innovation and direct communication across hierarchical levels. Such practices emphasize the importance of founders staying deeply involved in their companies' operations, challenging the traditional notion of delegating responsibilities to professional managers as companies scale.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "### Build Index\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "\n",
    "# Set embeddings\n",
    "embedding_model = OllamaEmbeddings(model='nomic-embed-text:v1.5', show_progress=True)\n",
    "\n",
    "#docs\n",
    "docs_list = [Document(page_content=sample_content, metadata={\"Title\": \"Paul Graham's Founder Mode Essay\", \"Source\": \"https://www.perplexity.ai/page/paul-graham-s-founder-mode-ess-t9TCyvkqRiyMQJWsHr0fnQ\"})]\n",
    "\n",
    "#Split\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=200, chunk_overlap=50\n",
    ")\n",
    "\n",
    "doc_splits = text_splitter.split_documents(docs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "for i, doc in enumerate(doc_splits):\n",
    "     doc.metadata['chunk_id'] = i+1  ### adding chunk id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Propositions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "# Data Model\n",
    "class GeneratePropositions(BaseModel):\n",
    "      \"\"\"List of all the propositions in a given document\"\"\"\n",
    "\n",
    "      propositions: List[str] = Field(\n",
    "        description = \"List of propositions (factual, self-contained and concise information)\"\n",
    "      )\n",
    "\n",
    "# LLM with function call\n",
    "llm = ChatGroq(model=\"llama-3.1-70b-versatile\", temperature=0)\n",
    "structured_llm = llm.with_structured_output(GeneratePropositions)\n",
    "\n",
    "#Few shot prompting --- We can add more examples to make it good\n",
    "propositions_examples = [\n",
    "    {\"document\": \n",
    "        \"In 1969, Neil Armstrong became the first person to walk on the Moon during the Apollo 11 mission.\", \n",
    "     \"propositions\": \n",
    "        \"['Neil Armstrong was an astronaut.', 'Neil Armstrong walked on the Moon in 1969.', 'Neil Armstrong was the first person to walk on the Moon.', 'Neil Armstrong walked on the Moon during the Apollo 11 mission.', 'The Apollo 11 mission occurred in 1969.']\"\n",
    "    },\n",
    "]\n",
    "\n",
    "example_proposition_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"{document}\"),\n",
    "        (\"ai\", \"{propositions}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt = example_proposition_prompt,\n",
    "    examples = propositions_examples\n",
    ")\n",
    "\n",
    "#Prompt\n",
    "system = \"\"\"Please break down the following text into simple, self-contained propositions. Ensure that each proposition meets the following criteria:\n",
    "\n",
    "    1. Express a Single Fact: Each proposition should state one specific fact or claim.\n",
    "    2. Be Understandable Without Context: The proposition should be self-contained, meaning it can be understood without needing additional context.\n",
    "    3. Use Full Names, Not Pronouns: Avoid pronouns or ambiguous references; use full entity names.\n",
    "    4. Include Relevant Dates/Qualifiers: If applicable, include necessary dates, times, and qualifiers to make the fact precise.\n",
    "    5. Contain One Subject-Predicate Relationship: Focus on a single subject and its corresponding action or attribute, without conjunctions or multiple clauses.\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        few_shot_prompt,\n",
    "        (\"human\", \"{document}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "proposition_generator = prompt | structured_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "propositions = [] # Store all propositions from the document\n",
    "\n",
    "for i in range(len(doc_splits)):\n",
    "    response = proposition_generator.invoke({\"document\": doc_splits[i].page_content}) # Creating propositions\n",
    "    for proposition in response.propositions:\n",
    "        propositions.append(Document(page_content=proposition, metadata= {\"Title\": \"Paul Graham's Founder Mode Essay\", \"Source\": \"https://www.perplexity.ai/page/paul-graham-s-founder-mode-ess-t9TCyvkqRiyMQJWsHr0fnQ\", \"chunk_id\": i+1}))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quality Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Data Model\n",
    "\n",
    "class GradePropositions(BaseModel):\n",
    "       \"\"\"Grade a given proposition on accuracy, clarity, completeness and conciseness\"\"\"\n",
    "\n",
    "       accuracy: int = Field(\n",
    "        description = \"Rate from 1-10 based on how well the proposition reflects the original text.\"\n",
    "       )\n",
    "       clarity: int = Field(\n",
    "        description=\"Rate from 1-10 based on how easy it is to understand the proposition without additional context.\"\n",
    "       )\n",
    "       completeness: int = Field(\n",
    "        description=\"Rate from 1-10 based on whether the proposition includes necessary details (e.g., dates, qualifiers).\"\n",
    "       )\n",
    "       conciseness: int = Field(\n",
    "        description=\"Rate from 1-10 based on whether the proposition is concise without losing important information.\"\n",
    "       )\n",
    "\n",
    "#LLM with Function call\n",
    "llm = ChatGroq(model=\"llama-3.1-70b-versatile\", temperature=0)\n",
    "structured_llm = llm.with_structured_output(GradePropositions)\n",
    "\n",
    "#Prompt\n",
    "evaluation_prompt_template = \"\"\"\n",
    "Please evaluate the following proposition based on the criteria below:\n",
    "- **Accuracy**: Rate from 1-10 based on how well the proposition reflects the original text.\n",
    "- **Clarity**: Rate from 1-10 based on how easy it is to understand the proposition without additional context.\n",
    "- **Completeness**: Rate from 1-10 based on whether the proposition includes necessary details (e.g., dates, qualifiers).\n",
    "- **Conciseness**: Rate from 1-10 based on whether the proposition is concise without losing important information.\n",
    "\n",
    "Example:\n",
    "Docs: In 1969, Neil Armstrong became the first person to walk on the Moon during the Apollo 11 mission.\n",
    "\n",
    "Propositons_1: Neil Armstrong was an astronaut.\n",
    "Evaluation_1: \"accuracy\": 10, \"clarity\": 10, \"completeness\": 10, \"conciseness\": 10\n",
    "\n",
    "Propositons_2: Neil Armstrong walked on the Moon in 1969.\n",
    "Evaluation_3: \"accuracy\": 10, \"clarity\": 10, \"completeness\": 10, \"conciseness\": 10\n",
    "\n",
    "Propositons_3: Neil Armstrong was the first person to walk on the Moon.\n",
    "Evaluation_3: \"accuracy\": 10, \"clarity\": 10, \"completeness\": 10, \"conciseness\": 10\n",
    "\n",
    "Propositons_4: Neil Armstrong walked on the Moon during the Apollo 11 mission.\n",
    "Evaluation_4: \"accuracy\": 10, \"clarity\": 10, \"completeness\": 10, \"conciseness\": 10\n",
    "\n",
    "Propositons_5: The Apollo 11 mission occurred in 1969.\n",
    "Evaluation_5: \"accuracy\": 10, \"clarity\": 10, \"completeness\": 10, \"conciseness\": 10\n",
    "\n",
    "Format:\n",
    "Proposition: \"{proposition}\"\n",
    "Original Text: \"{original_text}\"\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", evaluation_prompt_template),\n",
    "        (\"human\", \"{proposition}, {original_text}\")\n",
    "    ]\n",
    ")\n",
    "proposition_evaluator = prompt | structured_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Define evaluation categories and thresholds\n",
    "evaluation_categories = [\"accuracy\", \"clarity\", \"completeness\", \"conciseness\"]\n",
    "thresholds = {\"accuracy\": 7, \"clarity\": 7, \"completeness\": 7, \"conciseness\": 7}\n",
    "\n",
    "# Function to evaluate proposition\n",
    "def evaluate_proposition(proposition, original_text):\n",
    "    response = proposition_evaluator.invoke({\"proposition\": proposition, \"original_text\": original_text})\n",
    "    \n",
    "    # Parse the response to extract scores\n",
    "    scores = {\"accuracy\": response.accuracy, \"clarity\": response.clarity, \"completeness\": response.completeness, \"conciseness\": response.conciseness}  # Implement function to extract scores from the LLM response\n",
    "    return scores\n",
    "\n",
    "# Check if the proposition passes the quality check\n",
    "def passes_quality_check(scores):\n",
    "    for category, score in scores.items():\n",
    "        if score < thresholds[category]:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "evaluated_propositions = [] # Store all the propositions from the document\n",
    "\n",
    "# Loop through generated propositions and evaluate them\n",
    "for idx, proposition in enumerate(propositions):\n",
    "    scores = evaluate_proposition(proposition.page_content, doc_splits[proposition.metadata['chunk_id'] - 1].page_content)\n",
    "    if passes_quality_check(scores):\n",
    "        # Proposition passes quality check, keep it\n",
    "        evaluated_propositions.append(proposition)\n",
    "    else:\n",
    "        # Proposition fails, discard or flag for further review\n",
    "        print(f\"{idx+1}) Propostion: {proposition.page_content} \\n Scores: {scores}\")\n",
    "        print(\"Fail\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Propositions in a vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#Add to vectorstore\n",
    "vectorstore_propositions = FAISS.from_documents(evaluated_propositions, embedding_model)\n",
    "retriever_propositions = vectorstore_propositions.as_retriever(\n",
    "    search_type = \"similarity\",\n",
    "    search_kwargs = {'k': 4}, # number of documents to retrieve\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "query = \"Who's management approach served as inspiartion for Brian Chesky's \\\"Founder Mode\\\" at Airbnb?\"\n",
    "res_proposition = retriever_propositions.invoke(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing performance with larger chunks size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Add to vectorstore_larger_\n",
    "vectorstore_larger = FAISS.from_documents(doc_splits, embedding_model)\n",
    "retriever_larger = vectorstore_larger.as_retriever(\n",
    "                search_type=\"similarity\",\n",
    "                search_kwargs={'k': 4}, # number of documents to retrieve\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "res_larger = retriever_larger.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "for i, r in enumerate(res_larger):\n",
    "    print(f\"{i+1}) Content: {r.page_content} --- Chunk_id: {r.metadata['chunk_id']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing\n",
    "\n",
    "#### Test - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "test_query_1 = \"what is the essay \\\"Founder Mode\\\" about?\"\n",
    "res_proposition = retriever_propositions.invoke(test_query_1)\n",
    "res_larger = retriever_larger.invoke(test_query_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "for i, r in enumerate(res_proposition):\n",
    "    print(f\"{i+1}) Content: {r.page_content} --- Chunk_id: {r.metadata['chunk_id']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "for i, r in enumerate(res_larger):\n",
    "    print(f\"{i+1}) Content: {r.page_content} --- Chunk_id: {r.metadata['chunk_id']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "test_query_2 = \"who is the co-founder of Airbnb?\"\n",
    "res_proposition = retriever_propositions.invoke(test_query_2)\n",
    "res_larger = retriever_larger.invoke(test_query_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "for i, r in enumerate(res_proposition):\n",
    "    print(f\"{i+1}) Content: {r.page_content} --- Chunk_id: {r.metadata['chunk_id']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "for i, r in enumerate(res_larger):\n",
    "    print(f\"{i+1}) Content: {r.page_content} --- Chunk_id: {r.metadata['chunk_id']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test - 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "test_query_3 = \"when was the essay \\\"founder mode\\\" published?\"\n",
    "res_proposition = retriever_propositions.invoke(test_query_3)\n",
    "res_larger = retriever_larger.invoke(test_query_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "for i, r in enumerate(res_proposition):\n",
    "    print(f\"{i+1}) Content: {r.page_content} --- Chunk_id: {r.metadata['chunk_id']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "for i, r in enumerate(res_larger):\n",
    "    print(f\"{i+1}) Content: {r.page_content} --- Chunk_id: {r.metadata['chunk_id']}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
