{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAPTOR: Recursive Abstractive Processing and Thematic Organization for Retrieval\n",
    "\n",
    "#### Overview\n",
    "\n",
    "###### RAPTOR is an advanced information retrieval and question-answering system that combines hierarchical document summarization, embedding-based retrieval, and contextual answer generation. It aims to efficiently handle large document collections by creating a multi-level tree of summaries, allowing for both broad and detailed information retrieval.\n",
    "\n",
    "#### Motivation\n",
    "\n",
    "###### Traditional retrieval systems often struggle with large document sets, either missing important details or getting overwhelmed by irrelevant information. RAPTOR addresses this by creating a hierarchical structure of the document collection, allowing it to navigate between high-level concepts and specific details as needed.\n",
    "\n",
    "#### Key Components\n",
    "\n",
    "###### 1. Tree Building: Creates a hierarchical structure of document summaries.\n",
    "###### 2.Embedding & Clustering: Organizes document and summaries based on semantic similarity.\n",
    "###### 3. Vectorstore: Efficiently stores and retrieves document and summary embeddings\n",
    "###### 4. Contextual Retriever: Selects the most relevant information for a given query.\n",
    "###### 5. Answer Generation: Produces coherent responses based on retrieved information\n",
    "\n",
    "#### Method Details\n",
    "\n",
    "##### Tree Building\n",
    "\n",
    "###### 1. Start with original documents at level 0.\n",
    "###### 2. For each level:\n",
    "######          -> Embed the texts using a language model\n",
    "######          -> Cluster the embeddings (e.g., using a Gaussian Mixture Models)\n",
    "######          -> Generate summaries for each cluster\n",
    "######          -> Use these summaries as the texts for the next level\n",
    "###### 3. Continue until reaching a single summary or a maximum level\n",
    "\n",
    "##### Embedding Retrieval\n",
    "\n",
    "###### 1. Embed all documents and summaries from all levels of the tree.\n",
    "###### 2. Store these embeeddings in a vectorstore (e.g, FAISS) for efficient similarity search.\n",
    "###### 3. For a given query:\n",
    "######        -> Embed the query\n",
    "######        -> Retrieve the most similar documents/summaries from the vectorstore\n",
    "\n",
    "##### Contextual Information\n",
    "\n",
    "###### 1. Take the retrieved document/summaries\n",
    "###### 2. Use a language model to extract only the most relevant parts for the given query.\n",
    "\n",
    "##### Answer Generation\n",
    "\n",
    "###### 1. Combine the relevant parts into a context.\n",
    "###### 2. Use a language model to generate an answer based on this context and the original query\n",
    "\n",
    "#### Benefits of this Approach\n",
    "\n",
    "##### 1. Scalability: Can handle large document collections by working with summaries at "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
