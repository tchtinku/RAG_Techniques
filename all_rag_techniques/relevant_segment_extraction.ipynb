{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relevant Segment Extraction (RSE)\n",
    "\n",
    "#### Overview\n",
    "\n",
    "###### Relevant segment extraction (RSE) is a method of reconstructing multi-chunk segments of contiguous text out of retrieved chunks. This step occurs after vector search (and optionally reranking), but before presenting the retrieved context to the LLM. This method ensures that nearby chunks are presented to the LLM in order they appear in the original document. It also adds in chunks that are not marked as relevant, but are sandwiched between highly relevant chunks, further improving the context provided to the LLM. This method provides a substantial improvement in RAG performance, as shown in the eval results presented at the end of this notebook.\n",
    "\n",
    "#### Motivation\n",
    "\n",
    "###### When chunking documents for RAG, choosing the right chunk size is an exercise in managing tradeoffs. Large chunks provide better context to the LLM than small chunks, but they also make it harder to precisely retrieve specific piece of information. Some queries (like simple factoid questions) are best handled by small chunks, while other queries (like higher-level questions) require very large chunks. There are some queries that can be answered with a single sentence from the document, while there are other queries that require entire sections or chapters to properly answer. Most real-world RAG use cases face a combination of these type of queries.\n",
    "\n",
    "###### What we really need is a more dynamic system that can retrieve short chunks when that's all that's needed, but can also retrieve very large chunks when required. How do we do that?\n",
    "\n",
    "###### Our solution is motivated by one simple insight: relevant chunks tend to be clustered within their original documents.\n",
    "\n",
    "#### Key Components\n",
    "\n",
    "##### Chunk text key-value store\n",
    "\n",
    "###### RSE requires being able to retrieve chunk text from a database quickly, using a doc_id and chunk_index as keys. This is because not all chunks that need to be included in a given segment will have been returned in the initial search results. Therefore some sort of key-value store may need to be used in addition to the vector database.\n",
    "\n",
    "#### Method Details\n",
    "\n",
    "##### Document Chunking\n",
    "\n",
    "###### Standard document chunking methods can be used. The only special requirement here is that documents are chunked with no overlap. This allow us to reconstruct sections of the document (i.e segments) by concatenating chunks.\n",
    "\n",
    "##### RSE Optimization\n",
    "\n",
    "###### After the standard chunk retrieval process is completed, which ideally includes a reranking step, the RSE process can begin. The first step is to combine the absolute relevance value (i.e the similarity score) and the relevance rank. This provides a more robust starting point than just using the similarity score on its own or just using the ranks on its own. Then we substract a contant threshold value (let's say 0.2) from each chunk's value, such that irrelevant chunks have a negative value (as low as -0.2), and relevant chunks have a positive value (as high as 0.8). By calculating chunk values this way we can define segment value as just the sum of the chunk values.\n",
    "\n",
    "###### For example suppose chunks 0-4 in a document have the following chunk values: [-0.2, -0.2, 0.4, 0.8]. The segment that includes only chunks 2-3 would have value 0.4+0.8=1.2\n",
    "\n",
    "###### Finding the best segments then becomes a constrained version of the maximum sum subarray problem. We use a brute force search with a few heuristics to make it efficient. This generally takes ~5-10ms\n",
    "\n",
    "#### Setup\n",
    "\n",
    "###### First, some setup. You'll need a Cohere API key to run some of these cells, as we use their excellent rerankers to calculate relevance scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from typing import List\n",
    "from scipy.stats import beta\n",
    "from matplotlib.pyplot as plt\n",
    "import cohere\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "#Load environment variables from a .env file\n",
    "load_dotenv()\n",
    "os.environ[\"CO_API_KEY\"] = os.getenv('CO_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### We define a few helper functions. We will use a cohere Rerank API to calculate relevance values for our chunks. Normally, we'd start with a vector and/or keyword search to narrow down the list of candidates, but since we're just dealing with a single document here we can send just all chunks directly to the reranker, keeping things a bit simpler. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSpliter\n",
    "\n",
    "def split_into_chunks(text: str, chunk_size: int):\n",
    "    \"\"\"\n",
    "    Split a given text into chunks of specified size using RecursiveCharacterTextSpliter.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text to be split into chunks.\n",
    "        chunk_size (int, optional): The maximum size of each chunk. Defaults to 800.\n",
    "\n",
    "    Returns:\n",
    "        list[str]: A list of text chunks\n",
    "    \n",
    "    Example: \n",
    "        >>> text = \"This is a sample text to be split into chunks.\"\n",
    "        >>> chunks = split_into_chunks(text, chunk_size=10)\n",
    "        >>> print(chunks)\n",
    "        ['This is a', 'sample', 'text to', 'be split', 'into', 'chunks.']\n",
    "    \"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSpliter(chunk_size, chunk_overlap=0, length_function=len)\n",
    "    texts = text_splitter.create_documents([text])\n",
    "    chunks = [text.page_content for text in texts]\n",
    "    return chunks\n",
    "\n",
    "def transform(x: float):\n",
    "    \"\"\"\n",
    "    Transformation function to map the absolute relevance value to a value that is more uniformly distributed between 0 and 1. The relevance values given by the Cohere reranker tend to be very close to 0 or 1. This beta function used here helps to spread out the values more uniformly.\n",
    "\n",
    "    Args:\n",
    "        x (float): The absolute relevance value returned by the Cohere reranker\n",
    "\n",
    "    Returns:\n",
    "        float: The transformed relevance value\n",
    "    \"\"\"\n",
    "    a, b = 0.4, 0.4 # These can be adjusted to change the distribution shape\n",
    "    return beta.cdf(x, a, b)\n",
    "\n",
    "def rerank_chunks(query: str, chunks: List[str]):\n",
    "    \"\"\"\n",
    "    Use Cohere Rerank API to rerank the search results\n",
    "\n",
    "    Args:\n",
    "        query (str): The search query\n",
    "        chunks (list): List of chunks to be reranked\n",
    "\n",
    "    Returns:\n",
    "        similarity_scores (list): List of similarity scores for each chunk\n",
    "        chunk_values (list): List of relevance values (fusion of rank and similarity) for each chunk\n",
    "    \"\"\"\n",
    "    model = \"rerank-english-v3.0\"\n",
    "    client = cohere.Client(api_key=os.environ[\"CO_API_KEY\"])\n",
    "    decay_rate = 30\n",
    "\n",
    "    reranked_results = client.rerank(model=model, query=query, documents=chunks)\n",
    "    results = reranked_results.results\n",
    "    reranked_indices = [result.index for result in results]\n",
    "    reranked_similarity_scores = [result.relevance_score for result in results] #in order of reranked_indices\n",
    "\n",
    "    #Convert back to order of original documents and calculate the chunk values\n",
    "    similarity_scores = [0]*len(chunks)\n",
    "    chunk_values = [0]*len(chunks)\n",
    "    for i, index in enumerate(reranked_indices):\n",
    "          absolute_relevance_value = transform(reranked_similarity_scores[i])\n",
    "          similarity_scores[index] = absolute_relevance_value\n",
    "          chunk_values[index] = np.exp(-i/decay_rate)*absolute_relevance_value # decay the relevance value based on the rabk\n",
    "    return similarity_scores, chunk_values\n",
    "\n",
    "def plot_relevance_scores(chunk_values: List[float], start_index: int = None, end_index: int = None) -> None:\n",
    "    \"\"\"\n",
    "    Visualize the relevance scores of each chunk in the document to the search query\n",
    "\n",
    "    Args:\n",
    "       chunk_values (list): List of relevance values for each chunk\n",
    "       start_index (int): Start index of the chunks to be plotted \n",
    "       end_index (int): End index of the chunks to be plotted \n",
    "    Returns:\n",
    "       None\n",
    "    Plots:\n",
    "       Scatter plot at the relevance scores of each chunk in the document to the search query\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.title(f\"similarity of each chunk in the document to the search query\")\n",
    "    plt.ylim(0, 1)\n",
    "    plt.xlabel(\"Chunk index\")\n",
    "    plt.ylabel(\"Quer-Chunk similarity\")\n",
    "    if start_index is None:\n",
    "       start_index = 0\n",
    "    if end_index is None:\n",
    "       end_index = len(chunk_values)\n",
    "    plt.scatter(range(start_index, end_index), chunk_values(start_index:end_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# File path for the input document\n",
    "FILE_PATH = \"../data/nike_2023_annual_report.txt\"\n",
    "\n",
    "with open(FILE_PATH, 'r') as file:\n",
    "     text = file.read()\n",
    "\n",
    "chunks = split_into_chunks(text, chunk_size=800)\n",
    "\n",
    "print(f\"Split the document into {len(chunks)} chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize chunk relevance distribution across single document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Example query that requires a longer result than a single chunk\n",
    "query = \"Nike consolidated financial statements\"\n",
    "\n",
    "similarity_scores, chunk_values = rerank_chunks(query, chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "plot_relevance_score(chunk_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to interpret the chunk relevance plot above\n",
    "###### In the plot above, the x-axis represents the chunk index. The first chunk in the document has index 0, the next chunk has index 1, etc. The y-axis represents the relevance of each chunk to the query. Viewing it this way lets us see how relevant chunks tend to be clustered in one or more sections of a document.\n",
    "\n",
    "###### Note: the relevance values in this plot are actually a combination of the raw relevance value and the relevance ranks. An exponential decay function is applied to the ranks, and that is then multiplied by the raw relevance value. Using this combination provides a more robust measure of relevance than using just one or the other.\n",
    "\n",
    "###### Zooming in\n",
    "###### Now let's zoom in on that cluster of relevant chunks for a closer look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "plot_relevance_scores(chunk_values, 320, 340)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### What's interesting to note here is that only 7 of these 20 chunks have been marked as relevant by our reranker. And many of the non-relevant chunks are sandwiched between relevant chunks. Looking at the span of 323-336, exactly half of those chunks are marked as relevant and the other half are marked as not relevant.\n",
    "\n",
    "#### Let's see what this part of the document contains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def print_document_segment(chunks: List[str], start_index: int, end_index: int):\n",
    "    \"\"\"\n",
    "    Print the text content of a segment of the document\n",
    "\n",
    "    Args:\n",
    "        chunks (list): List of text chunks\n",
    "        start_index (int): Start index of the segment\n",
    "        end_index (int): End index of the segment (not inclusive)\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "\n",
    "    Prints:\n",
    "        The text content of the specified segment of the document\n",
    "    \"\"\"\n",
    "\n",
    "    for i in range(start_index, end_index):\n",
    "         print(f\"\\nChunk {i}\")\n",
    "         print(chunks[i])\n",
    "\n",
    "\n",
    "print_document_segment(chunks, 320, 340)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### We can see that the Consolidated Statement of Income starts in chunk 323, and everything up to chunk 333 contains consolidated financial statements, which is what we're looking for. So every chunk in that range is indeed relevant and necessary for our query, yet only about half of those chunks were marked as relevant by the reranker. So in addition to providing more complete context to the LLM, by combining these clusters of relevant chunks we actually find important chunks that otherwise would have been ignored.\n",
    "\n",
    "#### What can we do with these clusters of relevant chunks?\n",
    "###### The core idea is that clusters of relevant chunks, in their original contiguous form, provide much better context to the LLM than individual chunks can. Now for the hard part: how do we actually identify these clusters?\n",
    "\n",
    "###### If we can calculate chunk values in such a way that the value of a segment is just the sum of the values of its constituent chunks, then finding the optimal segment is a version of the maximum subarray problem, for which a solution can be found relatively easily. How do we define chunk values in such a way? We'll start with the idea that highly relevant chunks are good, and irrelevant chunks are bad. We already have a good measure of chunk relevance, on a scale of 0-1, so all we need to do is subtract a constant threshold value from it. This will turn the chunk value of irrelevant chunks to a negative number, while keeping the values of relevant chunks positive. We call this the irrelevant_chunk_penalty. A value around 0.2 seems to work well empirically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def get_best_segments(relevance_values: list, max_length: int, overall_max_length: int, minimum_value: float):\n",
    "    \"\"\"\n",
    "    This function takes the chunk relevance values and then runs an optimization algorithm to find the best segments. In more technical terms, it solves a constrained version of the maximum sum subarray problem.\n",
    "\n",
    "    Note: this is a simplified implementation intended for demonstration purposes. A more sophisticated implementation would be needed for production use and is available in the dsRAG library.\n",
    "\n",
    "    Args:\n",
    "        relevance_values (list): a list of relevance values for each chunk of a document\n",
    "        max_length (int): the maximum length of a single segment (measured in number of chunks)\n",
    "        overall_max_length (int): the maximum length of all segments (measured in number of chunks)\n",
    "        minimum_value (float): the minimum value that a segment must have to be considered\n",
    "\n",
    "    Returns:\n",
    "        best_segments (list): a list of tuples (start, end) that represent the indices of the best segments (the end index is non-inclusive) in the document\n",
    "        scores (list): a list of the scores for each of the best segments\n",
    "    \"\"\"\n",
    "\n",
    "    best_segments = []\n",
    "    scores = []\n",
    "    total_length = 0\n",
    "    while total_length < overall_max_length:\n",
    "        # find the best remaining segment\n",
    "        best_segment = None\n",
    "        best_value = -1000\n",
    "        for start in range(len(relevance_values)):\n",
    "            #Skip over negative value starting points\n",
    "            if relevance_values[start] < 0:\n",
    "               continue\n",
    "            for end in range(start+1, min(start+max_length+1, len(relevance_values)+1)):\n",
    "                #Skip over negative value ending points\n",
    "                if relevance_values[end-1] < 0:\n",
    "                   continue\n",
    "                # Check if this segment overlaps with any of the best segments and skip if it does\n",
    "                if any(start < seg_end and end > seg_start, seg_end in best_segments):\n",
    "                   continue\n",
    "                #Check if this segment would push us over the overall max length and skip it if it would\n",
    "                if total_length + end - start > overall_max_length:\n",
    "                   continue\n",
    "\n",
    "                #Define segment value as the sum of the relevance values of its chunks\n",
    "                segment_value = sum(relevance_values[start:end])\n",
    "                if segment_value > best_value:\n",
    "                   best_value = segment_value\n",
    "                   best_segment = (start, end)\n",
    "\n",
    "        # If we didn't find a valid segment then we're done\n",
    "        if best_segment is None or best_value < minimum_value:\n",
    "           break\n",
    "\n",
    "        #otherwise, add the segment to the list of best segments\n",
    "        best_segments.append(best_segment)\n",
    "        scores.append(best_value)\n",
    "        total_length += best_segment[1] - best_segment[0]\n",
    "\n",
    "return best_segments, scores        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#define some parameters and constraints for the optimization\n",
    "\n",
    "irrelevant_chunk_penalty = 0.2 # empirically, something around 0.2 works well; lower values bias towards segments\n",
    "max_length = 20\n",
    "overall_max_length = 30\n",
    "minimum_value = 0.7 \n",
    "\n",
    "#Substract constant threshold value from chunk relevance values\n",
    "relevance_values = [v - irrelevant_chunk_penalty for v in chunk_values]\n",
    "\n",
    "#run the optimization\n",
    "best_segments, scores = get_best_segments(relevance_values, max_length, overall_max_length, minimum_value)\n",
    "\n",
    "#print results\n",
    "print(\"Best segment indices\")\n",
    "print(best_segments) #Indices of the best segments, with the end index non-inclusive\n",
    "print()\n",
    "print(\"Best segment scores\")\n",
    "print(scores)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### The first segment given by the optimization algorithm is chunks 323-336. Looking at the chunks manually, we decided that 323-333 was the ideal segment, so we got a few bonus chunks that we don't really need, but overall this is going to be a great piece of context for the LLM to work with. We also identified some shorter segments from other parts of the document that we could provide to the LLM as well.\n",
    "\n",
    "#### What if the answer is contained in a single chunk?\n",
    "###### In the case where only a single chunk, or a few isolated chunks, are relevant to the query, we don't want to create large segments out of them. We just want to return those specific chunks. RSE can handle that scenario well too. Since there are no clusters of relevant chunks, it basically reduces to standard top-k retrieval in that case. We'll leave it as an exercise to the reader to see what happens to the chunk relevance plot and the resulting best segments for queries like this.\n",
    "\n",
    "#### Eval results\n",
    "#### KITE\n",
    "###### We evaluated RSE on an end-to-end RAG benchmark we created, called KITE (Knowledge-Intensive Task Evaluation).\n",
    "\n",
    "###### KITE currently consists of 4 datasets and a total of 50 questions.\n",
    "\n",
    "###### AI Papers - ~100 academic papers about AI and RAG, downloaded from arXiv in PDF form.\n",
    "###### BVP Cloud 10-Ks - 10-Ks for all companies in the Bessemer Cloud Index (~70 of them), in PDF form.\n",
    "###### Sourcegraph Company Handbook - ~800 markdown files, with their original directory structure, downloaded from Sourcegraph's publicly accessible company handbook GitHub page.\n",
    "###### Supreme Court Opinions - All Supreme Court opinions from Term Year 2022 (delivered from January '23 to June '23), downloaded from the official Supreme Court website in PDF form.\n",
    "###### Ground truth answers are included with each sample. Most samples also include grading rubrics. Grading is done on a scale of 0-10 for each question, with a strong LLM doing the grading.\n",
    "\n",
    "###### We compare RSE with standard Top-k retrieval (k=20). All other parameters remain the same between the two configurations. We use the Cohere 3 reranker, and we use GPT-4o for response generation. The average length of the relevant knowledge string is roughly the same between the two configurations, so cost and latency are similar.\n",
    "\n",
    "###### Top-k\tRSE\n",
    "###### AI Papers\t4.5\t7.9\n",
    "###### BVP Cloud\t2.6\t4.4\n",
    "###### Sourcegraph\t5.7\t6.6\n",
    "###### Supreme Court Opinions\t6.1\t8.0\n",
    "###### Average\t4.72\t6.73\n",
    "###### We can see that RSE leads to an improvement in performance on each of the four datasets. The overall average score increases from 4.72 -> 6.73, a 42.6% increase.\n",
    "\n",
    "#### FinanceBench\n",
    "###### We've also evaluated RSE on FinanceBench, where it contributed to a score of 83%, compared to a baseline score of 19%. For that benchmark, we tested contextual chunk headers (CCH) and RSE jointly, so we can't say exactly how much RSE contributed to that result. But the combination of CCH and RSE clearly leads to substantial accuracy improvements on FinanceBench."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
