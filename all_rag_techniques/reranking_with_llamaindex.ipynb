{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reranking Methods in RAG Systems\n",
    "\n",
    "#### Overview\n",
    "\n",
    "###### Reranking is a crucial step in Retrieval-Augmented Generation (RAG) systems that aims to improve the relevance and quality of retrieved documents. It involves reassessing and reordering initially retrieved documents to ensure that the most pertinent information is prioritized for subsequent processing or presentation.\n",
    "\n",
    "#### Motivation\n",
    "\n",
    "###### The primary motivation for reranking in RAG systems is to overcome limitations of initial retrieval methods, which often rely on simpler similarity metrics. Reranking allows for more sophisticated relevance assessment, taking into account nuanced relationships between queries and documents that might be missed by traditional retrieval techniques. This process aims to enhance the overall performance of RAG systems by ensuring that the most relevant information is used in the generation phase.\n",
    "\n",
    "#### Key Components\n",
    "\n",
    "###### Reranking systems typically include the following components:\n",
    "\n",
    "###### 1. Initial Retriever: Often a vector store using embeddings-based similarity search.\n",
    "###### 2. Reranking Model: This can be either:\n",
    "######     -> A Large Language Model (LLM) for scoring relevance\n",
    "######     -> A Cross-Encoder Model specifically trained for relevance assessment.\n",
    "###### 3. Scoring Mechanism: A method to assign relevance scores to documents.\n",
    "###### 4. Sorting and Selection Logic: To reorder document based on new scores.\n",
    "\n",
    "#### Method Details\n",
    "\n",
    "###### The reranking process generally follow these steps\n",
    "\n",
    "###### 1. Initial Retrieval: Fetch an initial set of potentially relevant documents.\n",
    "###### 2. Pair Creation: From query-document pairs for each retrieved document.\n",
    "###### 3. Scoring: \n",
    "######     -> LLM Method: Use prompts to ask the LLM to rate document relevance.\n",
    "######     -> Cross-Encoder Method: Feed query-document pairs directly into the model.\n",
    "###### 4. Score Interpretation: Parse and normalize the relevance scores\n",
    "###### 5. Reordering: Sort documents based on their new relevance scores\n",
    "###### 6. Selection: Choose the top k documents from the reordered list.\n",
    "\n",
    "#### Benefits of this Approach\n",
    "\n",
    "###### Reranking offers several advantages\n",
    "\n",
    "###### 1. Improved Relevance: By using more sophisticated models, reranking can capture subtle relevance factors.\n",
    "###### 2. Flexibility: Different reranking methods can be applied based on specific needs & resources.\n",
    "###### 3. Enhanced Context Quality: Providing more relevant documents to the RAG system improves the quality of generated responses.\n",
    "###### 4. Reduced Noise: Reranking helps filter out less relevant information, focuing on the most pertinent content.\n",
    "\n",
    "#### Conclusion\n",
    "\n",
    "###### Reranking is a powerful technique in  RAG systems that significantly enhances the quality of retrieved information. Whether using LLM-based scoring or specialized Cross-Encoder models, reranking allows for more nuanced and accurate assessment of document relevance. This improved relevance translates directly to better performance in downstream tasks, making reranking an essential component in advanced RAG implementations.\n",
    "\n",
    "###### The choice between LLM-based and Cross-Encoder reranking methods depends on factors such as required accuracy, available computational resources, and specific application needs. Both approaches offer substantial improvements over basic retrieval methods and contribute to the overall effectiveness of RAG systems.\n",
    "\n",
    "### Import Relevant Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "from typing import List\n",
    "from llama_index.core import Document\n",
    "from llama_index.core import Settings\n",
    "from llama_index.embeddings.openai import OpenAIEmbeddings\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core.readers import SimpleDirectoryReader\n",
    "from llama_index.vector_stores.faiss import FaissVectorStore\n",
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core.postprocessor import SentenceTransformerRerank, LLMRerank\n",
    "from llama_index.core import QueryBundle\n",
    "import faiss\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..'))) #Add the parent directory to the path since we work with notebooks\n",
    "\n",
    "#Load environment variables from a .env file\n",
    "load_dotenv()\n",
    "\n",
    "#Set the OpenAI API key environment variables\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "#Llama Index global Settings for llm & embeddings\n",
    "EMBED_DIMENSION=512\n",
    "Settings.llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
    "Settings.embed_model = OpenAIEmbeddings(model=\"text-embedding-3-small\", dimensions=EMBED_DIMENSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read Docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "path=\"../data/\"\n",
    "reader = SimpleDirectoryReader(input_dir=path, required_exts=['.pdf'])\n",
    "documents = reader.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Create FaisVectorStore to store embeddings\n",
    "fais_index = faiss.IndexFlatL2(EMBED_DIMENSION)\n",
    "vector_store = FaissVectorStore(faiss_index=fais_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ingestion Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "base_pipeline = IngestionPipeline(\n",
    "    transformations=[SentenceSplitter()],\n",
    "    vector_store=vector_store,\n",
    "    documents=documents\n",
    ")\n",
    "\n",
    "nodes = base_pipeline.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Querying\n",
    "\n",
    "##### Method 1: LLM based reranking the retrieved documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Create vector index from base nodes\n",
    "index = VectorStoreIndex(nodes)\n",
    "\n",
    "query_engine_w_llm_rerank = index.as_query_engine(\n",
    "    similarity_top_k = 10,\n",
    "    node_postprocessors=[\n",
    "        LLMRerank(\n",
    "            top_n = 5\n",
    "        )\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "resp = query_engine_w_llm_rerank.query(\"What are the impacts of climate change on biodiversity\")\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examples that demonstrate why we should use reranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "chunks = [\n",
    "    \"The capital of India is great.\",\n",
    "    \"The capital of India is huge.\",\n",
    "    \"The capital of India is beautiful.\",\n",
    "    \"\"\"Have you ever visited Delhi? It is a beautiful city where you can eat delicious food and see the Red Fort. I really enjoyed all the cities in India, but its cpital with the Red Fort is my Favorite city.\"\"\",\n",
    "    \"I really enjoyed my trip to New Delhi, India. The city is beautiful and the food is delicious. I would love to visit again. Such a great capital city.\" \n",
    "]\n",
    "docs = [Document(page_content=sentence) for sentence in chunks]\n",
    "\n",
    "def compare_rag_techniques(query: str, docs: List[Document] = docs) -> None:\n",
    "    docs = [Document(text=sentence) for sentence in chunks]\n",
    "    index = VectorStoreIndex.from_documents(docs)\n",
    "\n",
    "    print(\"Comparison of Retrieval Techniques\")\n",
    "    print(\"==================================\")\n",
    "    print(f\"Query: {query}\\n\")\n",
    "\n",
    "    print(\"Baseline Retrieval Result:\")\n",
    "    baseline_docs = index.as_retriever(similarity_top_k=5).retrieve(query)\n",
    "    for i, doc in enumerate(baseline_docs[:2]): # Get only the first 2 retrieved docs\n",
    "        print(f\"\\nDocument {i+1}:\")\n",
    "        print(doc.text)\n",
    "\n",
    "    print(\"\\nAdvanced Retrieval Result:\")\n",
    "    reranker = LLMRerank(\n",
    "        top_n=2,\n",
    "    )\n",
    "    advanced_docs = reranker.postprocess_nodes(\n",
    "        baseline_docs,\n",
    "        QueryBundle(query)\n",
    "    )\n",
    "    for i, doc in enumerate(advanced_docs):\n",
    "        print(f\"\\nDocument {i+1}:\")\n",
    "        print(doc.text)\n",
    "\n",
    "query = \"What is the capital of India?\"\n",
    "compare_rag_techniques(query, docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2: Cross Encoder models\n",
    "\n",
    "###### Llamaindex has builtin support for SBERT models that can be used directly as node postprocessor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "query_engine_w_cross_encoder = index.as_query_engine(\n",
    "    similarity_top_k=10,\n",
    "    node_postprocessors=[\n",
    "        SentenceTransformerRerank(\n",
    "            model='cross-encoder/ms-marco-MiniLM-L-6-v2',\n",
    "            top_n=5\n",
    "        )\n",
    "    ],\n",
    ")\n",
    "\n",
    "resp = query_engine_w_cross_encoder.query(\"What are the impacts of climate change on biodiversity?\")\n",
    "print(resp)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
