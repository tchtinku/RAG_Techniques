{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple RAG (Retrieval-Augmented generation) System for CSV files\n",
    "\n",
    "### Overview\n",
    "\n",
    "###### This code implements a basic Retrieval-Augmented Generation (RAG) system for processing and querying CSV documents. The system encodes the document content into a vector store, which can then be queries to retrieve relevant information.\n",
    "\n",
    "### CSV file Structure and Use Case\n",
    "\n",
    "###### The CSV file contains dummy customer data, comprising various attributes like first name, last name, company, etc. This dataset will be utilized for a RAG use case, facilitating the creation of a customer information Q&A system.\n",
    "\n",
    "### Key Components\n",
    "\n",
    "###### 1. Loading and Splitting csv files.\n",
    "###### 2. Vector store creation using FAISS and OpenAI embeddings.\n",
    "###### 3. Query engine setup for querying the processed documents.\n",
    "###### 4. Creating a question nd answer over the csv data.\n",
    "\n",
    "### Method Details\n",
    "\n",
    "#### Document Preprocessing\n",
    "\n",
    "###### 1. The csv is loaded using LlamaIndex's PagedCSVReader\n",
    "###### 2. A FAISS vector store is created from these embeddings for efficient similarity search\n",
    "\n",
    "### Query Engine Setup\n",
    "\n",
    "###### 1. A query engine is configured to fetch the most relevant chunks for a given query then answer the questions\n",
    "\n",
    "### Benefits of this Approach\n",
    "\n",
    "###### 1. Scalability: Can handle large documents by processing them in chunks\n",
    "###### 2. Flexibility: Easy to adjust parameters like chunk size and number of retrieved results\n",
    "###### 3. Efficiency: Utilizes FAISS for fast similarity search in high-dimensional spaces.\n",
    "###### 4. Integration with Advanced NLP: Uses OpenAI embeddings for the state-of-the-art text representation.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "###### This simple RAG system provides a solid foundation for building more complex information retrieval and question-answering systems. By encoding document content into a searchable vector store, it enables efficient retrieval of relevant information in response to queries. This approach is particularly useful for applications requiring quick access to specific information within a CSV file\n",
    "\n",
    "### Import & Environment Variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from llama_index.core.readers import SimpleDirectoryReader\n",
    "from llama_index.core import Settings\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.readers.file import PagedCSVReader\n",
    "from llama_index.vector_stores.faiss import FaissVectorStore\n",
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "from llama_index.core import VectorStoreIndex\n",
    "import faiss\n",
    "import os\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "#Load environment variables from a .env file\n",
    "load_dotenv()\n",
    "\n",
    "#Set the OpenAI API key environment variable\n",
    "os.environ[\"OpenAI_API_KEY\"]=os.getenv('OpenAI_API_KEY')\n",
    "\n",
    "#llama_index global Settings for llm and embeddings\n",
    "EMBED_DIMENSION=512\n",
    "Settings.llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
    "Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\", dimensions=EMBED_DIMENSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSV File Structure and Use Case\n",
    "\n",
    "###### The CSV file contains dummy customer data, comprising values attributes like first name, last name, company, etc. This dataset will be utilized for a RAG use case, facilitating the creation of a customer information Q&A system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "file_path = ('../data/customers-100.csv') # Insert the path of the csv file\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "#Preview the csv file\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Create FaisVectorStore to store embeddings\n",
    "\n",
    "fais_index = fais.IndexFlatL2(EMBED_DIMENSION)\n",
    "vector_store = FaisVectorStore(fais_index=fais_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Process CSV Data as Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "csv_reader = PagedCSVReader()\n",
    "\n",
    "reader = SimpleDirectoryReader(\n",
    "    input_files = [file_path],\n",
    "    file_extractor={\".csv\": csv_reader}\n",
    ")\n",
    "\n",
    "docs = reader.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#Check a simple chunk\n",
    "\n",
    "print(docs[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ingestion Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "pipeline = IngestionPipeline(\n",
    "    vector_store=vector_store,\n",
    "    documents=docs\n",
    ")\n",
    "\n",
    "nodes = pipeline.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Query Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "vector_store_index = VectorStoreIndex(nodes)\n",
    "query_engine = vector_store_index.as_query_engine(similarity_top_k=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query the rag bot with a question based on the CSV data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "response = query_engine.query(\"Which company does Sheryl Baxter work for?\")\n",
    "response.response"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
