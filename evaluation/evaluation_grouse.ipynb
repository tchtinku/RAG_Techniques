{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG Evaluation and Meta-Evaluation with GroUSE\n",
    "#### Overview\n",
    "###### This tutorial introduces GroUSE, a framework for evaluating Retrieval-Augmented Generation (RAG) pipelines, focusing on the final stage: Grounded Question Answering (GQA). It demonstrates how to use Large Language Models (LLMs) to assess GQA answers across four distinct metrics and guides you through customizing your own Judge LLM using GroUSE unit tests.\n",
    "\n",
    "#### Motivation\n",
    "###### Manually evaluating RAG pipeline outputs can be challenging. The GroUSE framework leverages LLMs with finely tuned prompts to address all potential failure modes in Grounded Question Answering. GroUSE unit tests are used to identify the most effective prompts to optimize the performance of these evaluators.\n",
    "\n",
    "#### Key Components\n",
    "###### Answer Relevancy evaluation\n",
    "###### Completeness evaluation\n",
    "###### Faithfulness evaluation\n",
    "###### Usefulness evaluation\n",
    "###### Judge LLM Customization\n",
    "\n",
    "#### Method Details\n",
    "##### The task we want to assess: Grounded Question Answering\n",
    "###### Grounded Question Answering (QA) is usually the last step of a RAG pipeline: given a question and a set of documents retrieved from the corpus, an LLM must generate an answer. We expect the LLM to cite which document each piece of information is coming from, as depicted below. When no precise answer is in the documents, the LLM should indicate it in its answer. In that case, if some related information is available in the documents, the LLM can add it to the answer to show the corpus is not completely off-topic with respect to the question.\n",
    "\n",
    "##### Evaluation Metrics\n",
    "###### Each answer is evaluated according to six metrics. The fisrt four metrics are evaluated with an evaluator LLM call. Positive acceptance and negative rejection are deducted from the first four.\n",
    "\n",
    "###### 1. Answer Relevancy\n",
    "###### Answer relevancy assesses the relevance of the information provided in the answer regarding the question, using a Likert scale (1 to 5).\n",
    "\n",
    "###### 2. Completeness\n",
    "###### Completeness uses a Likert scale (1 to 5) to evaluate whether all relevant information from the documents is present in the answer.\n",
    "\n",
    "###### 3. Faithfulness\n",
    "###### Faithfulness is a binary score that checks if all facts in the answer are accurate and correctly attributed to the corresponding document.\n",
    "\n",
    "###### 4. Usefulness\n",
    "###### When the answer states that no references can answer the question but additional information is provided, usefulness is a binary score that determines if the provided additional information is still useful.\n",
    "\n",
    "###### 5. Positive Acceptance\n",
    "###### Percentage of samples that responded when they were supposed to.\n",
    "\n",
    "###### 6. Negative Rejection\n",
    "###### Percentage of samples that refrained from responding when there is no context in the documents that allow to answer the question.\n",
    "\n",
    "#### Benefits of the approach\n",
    "###### The GroUSE framework comprehensively addresses the seven failure modes of Grounded Question Answering, providing a thorough evaluation of your RAG pipeline's final stage.\n",
    "\n",
    "#### Implementation details\n",
    "###### Answer Relevancy, Completeness, Faithfulness and Usefulness are evaluated using GPT-4 as the default model, as it was the best model we tested. Positive acceptance and negative rejection can be deducted from the answer relevancy and completeness results as these can have None values when no references contain answers to the question.\n",
    "\n",
    "#### Conclusion\n",
    "###### The GroUSE framework provides a comprehensive set of evaluation metrics to assess the performance of Grounded Question Answering models. By addressing seven key failure modes, it enables developers to thoroughly evaluate and improve their RAG pipelines. The use of LLM-based judges, such as GPT-4, automate this evaluation process. To tailor the framework to your specific needs, you can develop a custom LLM evaluator and validate its performance using GroUSE unit tests.\n",
    "\n",
    "### Tutorial\n",
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import nest_asyncio\n",
    "\n",
    "from grouse import (\n",
    "    EvaluationSample,\n",
    "    GroundedQAEvaluator,\n",
    "    meta_evaluate_pipeline\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Avoid nested asyncio loops inside notebooks (this line is not needed if you run the code in a Python script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup your API key\n",
    "#### For this tutorial, you will need access to the OpenAI API and get an OpenAI API key. You can get one here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = input(\"Add OpenAI API key\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize the evaluator\n",
    "###### The default model used is GPT-4. Prompts are adapted to this model, so if you want to have the best results, keep using the default model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "evaluator = GroundQAEvaluator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate a good answer\n",
    "###### An LLM has given a good answer to a question related to the Eiffel Tower, given some contexts from the Eiffel Tower Wikipedia page. Let's evaluate the answer and check that everything is okay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "good_sample = EvaluationSample(\n",
    "    input=\"Where is the Eiffel Tower located?\",\n",
    "    actual_output=\"The Eiffel Tower stands in the Champs de Mars in Paris.[1]\",\n",
    "    expected_output=\"In the Champs de Mars in Paris. [1]\",\n",
    "    references=[\n",
    "        \"The Eiffel Tower is a wrought-iron lattice tower on the Champ de Mars in Paris, France\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "result = evaluator.evaluate(eval_samples=[good_sample]).evaluations[0]\n",
    "\n",
    "print(\"Answer Relevancy (1 to 5):\", result.answer_relevancy.answer_relevancy)\n",
    "print(\"Answer Relevancy (1 to 5):\", result.answer_relevancy.answer_relevancy_justification)\n",
    "print(\"Completeness (1 to 5):\", result.completeness.completeness)\n",
    "print(\"Completeness (1 to 5):\", result.completeness.completeness_justification)\n",
    "print(\"Faithfulness (0 or 1):\", result.faithfulness.faithfulness)\n",
    "print(\"Faithfulness (0 or 1):\", result.faithfulness.faithfulness_justification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How does it behave with an irrelevant answer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "irrelevant_sample = EvaluationSample(\n",
    "    input=\"Where is the Eiffel Tower located?\",\n",
    "    actual_output=\"The Eiffel Tower is mainly made of puddle iron.[2]\",\n",
    "    expected_output=\"In the Champs de Mars in Paris.[1]\",\n",
    "    references=[\n",
    "        \"The Eiffel Tower is a wrought-iron lattice tower on the Champ de Mars in Paris, France\",\n",
    "        \"The puddle iron (wrought iron) of the Eiffel Tower weighs 7,300 tonnes,[70] and the addition of lifts, shops and antennae have brought the total weight to approximately 10,100 tonnes.\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "result = evaluator.evaluate(eval_samples=[irrelevant_sample]).evaluations[0]\n",
    "\n",
    "print(\"Answer Relevancy (1 to 5):\", result.answer_relevancy.answer_relevancy)\n",
    "print(\"Justification:\", result.answer_relevancy.answer_relevancy_justification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation of an incomplete sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "incomplete_sample = EvaluationSample(\n",
    "    input=\"Who critized the Eiffel Tower project in 1889?\",\n",
    "    actual_output=(\n",
    "        \"The tower was critized by those who did not believe it was feasible and some artists.[1]\"\n",
    "    ),\n",
    "    expected_output=(\n",
    "        \"The tower was critized by those who did not believe it was feasible and those who objected on artistic grounds.[1]\"\n",
    "        \"An artist committee was created to protest againt the construction of the tower, led by the prominent architect \"\n",
    "        \"Charles Garnier and including some of the most important figures of the arts, \"\n",
    "        \"such as William-Adolphe Bouguereau, Guy de Maupassant, Charles Gounod and Jules Massenet. [2]\"\n",
    "    ),\n",
    "    references=[\n",
    "        \"The proposed tower had been a subject of controversy, drawing criticism from those who did not believe it was feasible and those who objected on artistic grounds.\",\n",
    "        (\n",
    "            \"It came to a head as work began at the Champ de Mars: a \\\"Committee of Three Hundred\\\" \"\n",
    "            \"(one member for each metre of the tower's height) was formed, led by the prominent architect \"\n",
    "            \"Charles Garnier and including some of the most important figures of the arts, \"\n",
    "            \"such as William-Adolphe Bouguereau, Guy de Maupassant, Charles Gounod and Jules Massenet.\"\n",
    "        ),\n",
    "        \"A petition called \\\"Artists against the Eiffel Tower\\\" was sent to the Minister of Works and Commissioner for the Exposition, Adolphe Alphand, and it was published by Le Temps on 14 February 1887\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "result = evaluator.evaluate(eval_samples=[incomplete_sample]).evaluations[0]\n",
    "\n",
    "print(\"Completeness (1 to 5):\", result.completeness.completeness)\n",
    "print(\"Justification:\", result.completeness.completeness_justification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation of an unfaithful sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "unfaithful_sample = EvaluationSample(\n",
    "    input=\"Where is the Eiffel Tower located?\",\n",
    "    actual_output=\"The Eiffel Tower is located at Rue Rabelais in Paris.[1][2]\",\n",
    "    expected_output=\"In the Champs de Mars in Paris.[1]\",\n",
    "    references=[\n",
    "        \"The Eiffel Tower is a wrought-iron lattice tower on the Champ de Mars in Paris, France\",\n",
    "        \"Gustave Eiffel died in his appartment at Rue Rabelais in Paris.\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "result = evaluator.evaluate(eval_samples=[unfaithful_sample]).evaluations[0]\n",
    "\n",
    "print(\"Faithfulness (0 or 1):\", result.faithfulness.faithfulness)\n",
    "print(\"Justification:\", result.faithfulness.faithfulness_justification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation of information utility in case there is no answer to the question in the references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
